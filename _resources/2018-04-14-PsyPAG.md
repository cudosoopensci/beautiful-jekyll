---
layout: post
title: An Introduction and Practical Guide to Open Science Practices in Psychology
subtitle: PsyPAG 2018 workshop on incorporating Open Science practices into your research 
tags: [Open Science, Conferences] 
---

# Introduction 

This is the accompanying web page for the Open Science workshop at PsyPAG 2018. As the workshop is very short, hopefully a lot of you will be able to come prepared, and focus on doing some of the stuff in the workshop with guidance. You will then have this page to always refer back to!

I have prepared two different sections depending on how far along you are with a research project. It is easier to plan a project with Open Science from the start, but you can also make some changes at the end of a project too. Therefore, no matter where you are at with your studies, you should be able to leave the workshop having achieved some progress. 

## Who is this session aimed at? 

## What should you get out of this session? 

# Workshop activities

## For those planning a study 

## For those writing up a study 

# A brief history of the Open Science movement

There have been several earlier attempts at mapping out a brief history of the replicability crisis. For example, [Hilda Bastian](http://blogs.plos.org/absolutely-maybe/2016/12/05/reproducibility-crisis-timeline-milestones-in-tackling-research-reliability/) wrote a timeline but with greater focus on clinical research. There are also two pieces by Andrew Gelman. The [first](http://andrewgelman.com/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/) is a timeline of the replicability crisis that is centred on psychology, and the [second](http://andrewgelman.com/2016/09/22/why-is-the-scientific-replication-crisis-centered-on-psychology/) is a piece on why concerns of reproducibility and replicability appear to be more focused on psychological research than other fields. Finally, a longer analysis of a general statistical crisis in science can be found in Stijn Debrouwere’s [thesis](https://lib.ugent.be/fulltxt/RUG01/002/304/385/RUG01-002304385_2016_0001_AC.pdf). These offer a good overview of events that influenced the Open Science movement, but there are some particular highlights. 

Concern over the research practices commonly used in psychology go back decades. For example, Cohen (1962) showed that in several high profile social psychology journals, studies had only 18% power to detect small effects and 48% to detect moderate effects which are the most prevalent in social psychology. Despite this early warning, low statistical power is still prevalent in psychological research with the power of neuroscience studies estimated to be 21% ([Button et al. 2013](https://www.nature.com/articles/nrn3475)). Although there were early rumbling by authors such as Cohen, one of the major turning points that stimulated a cultural shift in psychology was the publication of Daryl Bem’s “[feeling the future](https://www.ncbi.nlm.nih.gov/pubmed/21280961)” article. This was published in one of the most prestigious, selective journals for social psychology and contained nine studies purporting to show evidence of precognition, or the ability to see into the future. Eight of out nine studies showed evidence in favour of precognition which caused an uproar. One possibility is that the laws of physics and biology as we know them are wrong. The alternative is that there are deep flaws in the research methods commonly used in psychological research, and that it took a study so absurd for people to realise. An excellent journalistic take on this realisation was written by Daniel Engber on [Slate](https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html). This article also came after the damning essay by [John Ioannidis (2005)](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124#s6) who argued that most published research findings are wrong. One of his major points is that the greater the flexibility in the design, outcomes, and analyses, the more likely it is for a finding to be false. Finally, arguably the turning point for stimulating the Open Science movement for many people was the huge [Open Science Collaboration (2015)](http://science.sciencemag.org/content/349/6251/aac4716) between 270 authors trying to replicate 100 studies. Although there are still arguments over how to interpret the results, approximately 60% of studies failed to replicate despite using the materials from the original studies and utilising greater than 90% statistical power. 

We have seen early warnings of the research practices used in psychology, and a climax culminating in a crisis in confidence with Bem’s precognition work and a lack of replicable results. However, positives can be drawn from these realisations and we have potentially passed the crisis period. In the annual review of psychology, [Nelson et al. (2018)](http://www.annualreviews.org/doi/pdf/10.1146/annurev-psych-122216-011836#.WfhteR0R66c.twitter) call this “Psychology’s Renaissance’. We now have the opportunity to learn from these mistakes with many researchers starting to show how we can utilise better research practices rather than using all their energy condemning previous ones. For example, [Munafó et al. (2017)](https://www.nature.com/articles/s41562-016-0021) outlined a ‘manifesto for reproducible science’ that presents the hypothetico-deductive model of science (image below), and highlights how each step can be negatively affected by current research practices. However, it also makes suggestions on how you can use better research practices. For instance, there is often poor quality control in reporting research (e.g. statistics being misreported). Articles can be written within the statistical programme R to be reproducible, or mistakes can be more easily identified and rectified if the analysis scripts and data are shared.
